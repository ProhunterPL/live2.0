#!/usr/bin/env python3
"""
Quick script to check which Taichi backend is currently being used
"""

import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import taichi as ti

print("="*70)
print("TAICHI BACKEND DETECTION")
print("="*70)

# Try to detect what's available
print("\n1Ô∏è‚É£ Checking available backends...")

backends_available = []

# Check CUDA
print("\n  Testing CUDA (NVIDIA GPU)...")
try:
    ti.reset()
    ti.init(arch=ti.cuda, device_memory_GB=2.0)
    print("    ‚úÖ CUDA is AVAILABLE")
    backends_available.append("cuda")
    ti.reset()
except Exception as e:
    print(f"    ‚ùå CUDA not available: {str(e)[:60]}")

# Check Vulkan
print("\n  Testing Vulkan (Generic GPU)...")
try:
    ti.reset()
    ti.init(arch=ti.vulkan, device_memory_GB=2.0)
    print("    ‚úÖ Vulkan is AVAILABLE")
    backends_available.append("vulkan")
    ti.reset()
except Exception as e:
    print(f"    ‚ùå Vulkan not available: {str(e)[:60]}")

# CPU is always available
print("\n  Testing CPU...")
try:
    ti.reset()
    import multiprocessing
    max_threads = multiprocessing.cpu_count()
    ti.init(arch=ti.cpu, cpu_max_num_threads=max_threads)
    print(f"    ‚úÖ CPU is AVAILABLE ({max_threads} threads)")
    backends_available.append("cpu")
    ti.reset()
except Exception as e:
    print(f"    ‚ùå CPU error: {str(e)[:60]}")

# Check what the server.py uses
print("\n2Ô∏è‚É£ Checking what backend/api/server.py will use...")
print("   (based on server.py lines 56-65)")

if "cuda" in backends_available:
    print("    ‚Üí Will use: CUDA (GPU)")
    current_backend = "CUDA"
elif "vulkan" in backends_available:
    print("    ‚Üí Will use: Vulkan (GPU)")
    current_backend = "Vulkan"
elif "cpu" in backends_available:
    print("    ‚Üí Will use: CPU")
    current_backend = "CPU"
else:
    print("    ‚Üí ERROR: No backend available!")
    current_backend = "NONE"

# Summary
print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"\n‚úÖ Available backends: {', '.join(backends_available) if backends_available else 'NONE'}")
print(f"‚úÖ Current backend: {current_backend}")

# Recommendations
print("\n" + "="*70)
print("RECOMMENDATIONS")
print("="*70)

if "cuda" in backends_available:
    print("\nüöÄ You have CUDA (NVIDIA GPU)!")
    print("   For best performance, GPU is usually 10-50x faster than CPU")
    print("   Run the benchmark to confirm: python tests/benchmark_gpu_vs_cpu.py")
elif "vulkan" in backends_available:
    print("\nüöÄ You have Vulkan (Generic GPU)!")
    print("   GPU should be faster than CPU for most operations")
    print("   Run the benchmark to confirm: python tests/benchmark_gpu_vs_cpu.py")
else:
    print("\n‚ö†Ô∏è  No GPU detected, using CPU")
    print("   CPU can be fast with many threads, but typically slower than GPU")
    print("   Consider running on a machine with NVIDIA GPU for better performance")

print("\nüí° To run full benchmark:")
print("   PowerShell: .\\run_benchmark.ps1")
print("   Python: python tests/benchmark_gpu_vs_cpu.py")

print("\n" + "="*70)

